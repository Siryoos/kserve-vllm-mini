1. Get the benchmark harness pod name:
   kubectl get pods --namespace {{ .Release.Namespace }} -l "app.kubernetes.io/name={{ include "kvmini.name" . }},app.kubernetes.io/instance={{ .Release.Name }}"

2. Execute a benchmark run:
   {{- $global := .Values.global | default dict -}}
   {{- $globalIsvc := $global.isvc | default dict -}}
   {{- $globalModel := $global.model | default dict -}}
   {{- $ns := default $globalIsvc.namespace .Values.benchmark.defaults.namespace -}}
   {{- $svc := default "my-llm" $globalIsvc.name -}}
   {{- $model := default $globalModel.storageUri .Values.benchmark.model.uri -}}
   kubectl exec -n {{ .Release.Namespace }} deploy/{{ include "kvmini.fullname" . }} -- kvmini bench \
     --namespace {{ $ns | default "default" }} \
     --service {{ $svc }} \
     --model {{ $model | default "placeholder" }} \
     --requests {{ .Values.benchmark.defaults.requests }} \
     --concurrency {{ .Values.benchmark.defaults.concurrency }}

3. Check benchmark results:
   kubectl exec -n {{ .Release.Namespace }} deploy/{{ include "kvmini.fullname" . }} -- ls -la /data/runs/

4. Copy results locally:
   kubectl cp {{ .Release.Namespace }}/{{ include "kvmini.fullname" . }}:/data/runs ./results

{{- if .Values.persistence.enabled }}
5. Results are persisted to PVC: {{ include "kvmini.fullname" . }}-data
{{- end }}

For more information, see: https://github.com/kserve/kserve-vllm-mini
