name: "tensorrt_llm_bls"
backend: "tensorrtllm"
max_batch_size: 64

input [
  { name: "text_input" data_type: TYPE_BYTES dims: [1] },
  { name: "max_tokens" data_type: TYPE_UINT32 dims: [1] },
  { name: "bad_words" data_type: TYPE_BYTES dims: [1] },
  { name: "stop_words" data_type: TYPE_BYTES dims: [1] }
]

output [
  { name: "text_output" data_type: TYPE_BYTES dims: [1] },
  { name: "output_lengths" data_type: TYPE_INT32 dims: [1] }
]

parameters: {
  key: "tokenizer_type"
  value: { string_value: "auto" }
}

# NOTE: Place your TRT-LLM engines in this model version directory.
# The exact files and structure depend on how you built engines (mono vs multi-plan).
# See NVIDIA docs for your TRT-LLM version.
