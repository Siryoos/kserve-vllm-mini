# TensorRT-LLM engine profile for Phi-2.7B-class models

profile: trtllm-phi-2.7b-fp16
model_family: phi
model_name: phi-2.7b
weights_format: huggingface

dtype: fp16
quantization: none  # none | awq | gptq
kv_cache_dtype: fp16

tensor_parallel_size: 1
pipeline_parallel_size: 1
max_batch_size: 128      # smaller model â†’ higher batch
max_input_len: 4096
max_output_len: 512
max_beam_width: 1

builder_flags:
  use_gpt_attention_plugin: true
  use_context_fmha: true
  remove_input_padding: true
  use_gemm_plugin: true
  enable_ragged_kv_cache: true
  workspace_size_mb: 2048

runtime_env:
  TENSOR_PARALLEL_SIZE: "1"
  PIPELINE_PARALLEL_SIZE: "1"
  MAX_BATCH_SIZE: "128"
  MAX_INPUT_LEN: "4096"
  MAX_OUTPUT_LEN: "512"
  MAX_BEAM_WIDTH: "1"
  KV_CACHE_PRECISION: "fp16"

notes: |
  - Phi family is small; you can typically push batch sizes higher.
  - AWQ/GPTQ weight-only quantization can further improve throughput with
    minimal accuracy loss for many tasks.
